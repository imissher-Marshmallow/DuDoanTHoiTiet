# ü§ñ AI System Improvements & Configuration Guide

## Overview

The FloodSense AI system has been significantly enhanced with:
- **Conversation History**: Multi-turn dialogue with context awareness
- **Comprehensive Logging**: Track all AI interactions with timestamps
- **Real-time Water Context**: Latest sensor/forecast/history data automatically injected
- **Session Management**: Per-user conversation tracking
- **Enhanced Error Handling**: Detailed error messages and recovery

---

## üéØ New AI Features

### 1. Conversation History
The AI now remembers previous messages in a conversation, allowing for:
- Follow-up questions without repeating context
- Better understanding of user intent
- Consistent conversation flow

**How it works**:
```python
# Last 10 messages (default) stored per session
conversation_history[session_id] = [
    {'role': 'user', 'content': 'M·ª±c n∆∞·ªõc hi·ªán t·∫°i bao nhi√™u?'},
    {'role': 'assistant', 'content': 'M·ª±c n∆∞·ªõc hi·ªán t·∫°i l√† 125mm...'},
    {'role': 'user', 'content': 'S·∫Ω ng·∫≠p kh√¥ng?'},
    {'role': 'assistant', 'content': 'D·ª±a tr√™n d·ª± b√°o ML...'}
]
```

**Configuration**:
```bash
# In .env file
CONVERSATION_HISTORY_LIMIT=10  # Number of message pairs to keep
```

### 2. Real-time Water Context

Every AI response includes fresh water data:

**üìä Current Sensor Data**
- Real-time water level from Arduino
- Distance measurement from ultrasonic sensor
- Last update timestamp

**üîÆ ML Forecasts**
- 10-minute ahead prediction
- 30-minute ahead prediction
- Training sample count (confidence indicator)

**üìà Historical Statistics**
- Min/max/average water levels
- Trend direction (increasing/decreasing)
- Total number of historical records

**‚öôÔ∏è Sensor Configuration**
- Alert threshold setting
- Update interval configuration

### 3. Enhanced System Prompt

The AI now uses a sophisticated system prompt that:

```
B·∫°n l√† tr·ª£ l√Ω AI chuy√™n C·∫¢NH B√ÅO L≈® L·ª§T Vi·ªát Nam - FloodSense System.

**H∆∞·ªõng d·∫´n H√†nh ƒê·ªông:**
- Tr·∫£ l·ªùi ng·∫Øn g·ªçn, h·ªØu √≠ch b·∫±ng ti·∫øng Vi·ªát (t·ªëi ƒëa 3 c√¢u)
- D·ª±a tr√™n d·ªØ li·ªáu m·ªõi nh·∫•t t·ª´ c·∫£m bi·∫øn IoT v√† m√¥ h√¨nh ML
- S·ª≠ d·ª•ng d·ªØ li·ªáu th·ª±c t·∫ø d∆∞·ªõi ƒë√¢y
- C·∫£nh b√°o theo c·∫•p ƒë·ªô: <150mm (‚úì), 150-200mm (‚ö†Ô∏è), >200mm (üö®)
- Kh√¥ng b·ªãa ƒë·∫∑t d·ªØ li·ªáu - n·∫øu kh√¥ng c√≥, n√≥i r√µ "D·ªØ li·ªáu ch∆∞a c√≥"
```

### 4. Logging All Interactions

Every AI interaction is logged with:
- **Timestamp**: Exact time of interaction
- **User ID**: Session/IP address
- **Message**: User's input message
- **Response**: AI's reply
- **Metadata**: Model used, tokens consumed

**Log Example**:
```
2024-11-19 15:45:23,456 - FloodSense - INFO - [chat:245] - [CHAT_MESSAGE] Session:192.168.1.100 | User: m·ª±c n∆∞·ªõc hi·ªán t·∫°i bao nhi√™u?
2024-11-19 15:45:25,789 - FloodSense - INFO - [chat:290] - [CHAT_RESPONSE] Session:192.168.1.100 | AI: M·ª±c n∆∞·ªõc hi·ªán t·∫°i l√† 125mm, trong ng∆∞·ª°ng b√¨nh th∆∞·ªùng...
```

---

## üîß Configuration Options

### OpenAI Settings

```bash
# Model selection
AI_MODEL=gpt-4o-mini  # Options: gpt-4o, gpt-4o-mini, gpt-3.5-turbo

# Response quality
AI_MAX_TOKENS=300     # Max tokens per response (increase for longer answers)
AI_TEMPERATURE=0.7    # Creativity: 0.0 (deterministic) to 1.0 (creative)
```

**Temperature Guidance**:
- `0.0-0.3`: Strict, factual responses (best for water level data)
- `0.4-0.7`: Balanced, natural responses (current setting)
- `0.8-1.0`: Creative, varied responses (not recommended for alerts)

### Conversation Settings

```bash
# Session management
CONVERSATION_HISTORY_LIMIT=10  # Messages to keep per user
SESSION_TIMEOUT=3600            # Session expiration (seconds)

# Logging
LOG_LEVEL=INFO                  # DEBUG, INFO, WARNING, ERROR
LOG_DIR=./logs                  # Where to store log files
ENABLE_LOGGING=1                # 1 to enable, 0 to disable
```

### Model Comparison

| Model | Speed | Cost | Quality | Recommendation |
|-------|-------|------|---------|-----------------|
| gpt-3.5-turbo | ‚ö°‚ö°‚ö° | $ | Good | Quick responses |
| gpt-4o-mini | ‚ö°‚ö° | $$ | Excellent | **Current** (best value) |
| gpt-4o | ‚ö° | $$$ | Excellent | Premium/enterprise |
| gpt-4 | ‚ö° | $$$$ | Best | Maximum accuracy |

---

## üß™ Testing the AI System

### Test 1: Basic Water Query
```bash
curl -X POST http://localhost:5000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "M·ª±c n∆∞·ªõc hi·ªán t·∫°i bao nhi√™u?"}'
```

**Expected Response**:
```json
{
  "reply": "M·ª±c n∆∞·ªõc hi·ªán t·∫°i l√† 125mm, trong ng∆∞·ª°ng b√¨nh th∆∞·ªùng (< 150mm). D·ª± b√°o 10 ph√∫t t·ªõi s·∫Ω l√† ~130mm.",
  "status": "success",
  "timestamp": "2024-11-19T15:45:23.456789"
}
```

### Test 2: Follow-up Question (Tests History)
```bash
# First message
curl -X POST http://localhost:5000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "M·ª±c n∆∞·ªõc hi·ªán t·∫°i bao nhi√™u?"}'

# Follow-up (should reference previous answer)
curl -X POST http://localhost:5000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "S·∫Ω ng·∫≠p kh√¥ng?"}'
```

**Expected**: Second response should reference the water level mentioned in first response, showing history is working.

### Test 3: Danger Scenario
```bash
# Simulate by manually setting high water level in Firebase
curl -X POST http://localhost:5000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "M·ª±c n∆∞·ªõc hi·ªán t·∫°i l√† bao nhi√™u? Nguy hi·ªÉm kh√¥ng?"}'
```

**Expected Response** (if water > 200mm):
```
üö® NGUY HI·ªÇM! M·ª±c n∆∞·ªõc ƒë√£ v∆∞·ª£t ng∆∞·ª°ng c·∫£nh b√°o (>200mm). C·∫£nh b√°o! Di t·∫£n ngay!
```

### Test 4: No Data Scenario
```bash
curl -X POST http://localhost:5000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "D·ª± b√°o ti·∫øp theo s·∫Ω bao nhi√™u?"}'
```

**Expected**: Should mention "Ch∆∞a c√≥ d·ªØ li·ªáu d·ª± b√°o" if no forecast data available.

---

## üìä Monitoring AI Performance

### View Recent AI Conversations

```bash
# Get last 100 log lines
curl http://localhost:5000/api/logs | jq '.logs[-20:]'

# Or directly from file
tail -50 ./logs/app.log | grep CHAT
```

### Key Metrics to Monitor

1. **Response Time**
   ```bash
   grep "CHAT_RESPONSE" ./logs/app.log | \
   awk -F'|' '{print $1}' | \
   head -20
   ```

2. **Error Rate**
   ```bash
   grep "CHAT_ERROR" ./logs/app.log | wc -l
   ```

3. **Active Sessions**
   ```bash
   grep "CHAT_MESSAGE" ./logs/app.log | \
   cut -d: -f7 | sort -u | wc -l
   ```

4. **Model Costs** (estimate)
   ```bash
   # gpt-4o-mini: ~$0.00015 per 1K input tokens, ~$0.0006 per 1K output tokens
   # Average: ~$0.001 per chat (300 tokens)
   ```

---

## üéì AI Response Examples

### Water Level Questions

**User**: "M·ª±c n∆∞·ªõc hi·ªán t·∫°i bao nhi√™u?"

**AI Response**:
> M·ª±c n∆∞·ªõc hi·ªán t·∫°i l√† 125mm, trong ng∆∞·ª°ng b√¨nh th∆∞·ªùng (< 150mm). D·ª± b√°o 10 ph√∫t t·ªõi s·∫Ω l√† ~130mm.

### Risk Assessment

**User**: "S·∫Ω x·∫£y ra l≈© l·ª•t kh√¥ng?"

**AI Response**:
> D·ª±a tr√™n d·ªØ li·ªáu hi·ªán t·∫°i (125mm), b·∫°n v·∫´n an to√†n. Tuy nhi√™n, n·∫øu m·ª©c n∆∞·ªõc ti·∫øp t·ª•c tƒÉng v∆∞·ª£t qu√° 200mm, c·∫ßn c·∫£nh b√°o. Theo d√µi li√™n t·ª•c!

### Historical Analysis

**User**: "M·ª±c n∆∞·ªõc b√¨nh th∆∞·ªùng l√† bao nhi√™u?"

**AI Response**:
> D·ª±a tr√™n l·ªãch s·ª≠: m·ª±c n∆∞·ªõc trung b√¨nh 95mm, th·∫•p nh·∫•t 40mm, cao nh·∫•t 220mm. M·ª©c 125mm hi·ªán t·∫°i cao h∆°n trung b√¨nh nh∆∞ng v·∫´n an to√†n.

### Trend Analysis

**User**: "M·ª±c n∆∞·ªõc c√≥ tƒÉng kh√¥ng?"

**AI Response**:
> C√≥, xu h∆∞·ªõng hi·ªán t·∫°i l√† tƒÉng. T·ª´ trung b√¨nh 95mm l√™n 125mm. D·ª± b√°o ti·∫øp t·ª•c tƒÉng ~5mm m·ªói 10 ph√∫t.

---

## üõ†Ô∏è Advanced Customization

### Custom System Prompt

To modify AI behavior, edit the `build_water_context()` function in `backend/app.py`:

```python
system_prompt = f'''B·∫°n l√† tr·ª£ l√Ω AI chuy√™n C·∫¢NH B√ÅO L≈® L·ª§T Vi·ªát Nam.

**T√πy ch·ªânh h∆∞·ªõng d·∫´n:**
- Th√™m quy t·∫Øc ri√™ng ·ªü ƒë√¢y
- Thay ƒë·ªïi tone/style n·∫øu c·∫ßn
- ƒêi·ªÅu ch·ªânh alert thresholds

{water_context}
'''
```

### Custom Alert Thresholds

In `ketnoiphancung.html`:

```javascript
const ALERT_THRESHOLD = 200; // mm - thay ƒë·ªïi gi√° tr·ªã n√†y

if (sensor.waterLevel >= threshold) {
    showAlert('üö® NGUY HI·ªÇM! ...', 0, 'danger');
} else if (sensor.waterLevel >= threshold * 0.75) {
    showAlert('‚ö†Ô∏è CH√ö √ù: M·ª±c n∆∞·ªõc cao...', 5000, 'warning');
}
```

### Rate Limiting (Production)

Add to `backend/app.py`:

```python
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address

limiter = Limiter(
    app=app,
    key_func=get_remote_address,
    default_limits=["200 per day", "50 per hour"]
)

@app.route('/chat', methods=['POST'])
@limiter.limit("10 per minute")
def chat():
    # ... existing code ...
```

---

## üêõ Debugging AI Issues

### Issue: "AI service not configured"
**Solution**: Verify `OPENAI_API_KEY` in `.env`:
```bash
echo $OPENAI_API_KEY
# Should output: sk-proj-...
```

### Issue: "Slow AI responses"
**Solution**:
1. Reduce `AI_MAX_TOKENS` (currently 300)
2. Use cheaper model: `AI_MODEL=gpt-3.5-turbo`
3. Monitor API status: https://status.openai.com

### Issue: "AI gives generic responses"
**Solution**:
1. Increase `AI_TEMPERATURE` to 0.9 for more variety
2. Add more specific examples to system prompt
3. Check if water_context is empty

### Issue: "Memory usage increasing"
**Solution**:
1. Reduce `CONVERSATION_HISTORY_LIMIT` (currently 10)
2. Monitor sessions: `grep "CHAT_MESSAGE" ./logs/app.log | wc -l`
3. Clear old logs: `rm ./logs/app.log.* `

---

## üìà Cost Estimation

Using gpt-4o-mini (current):

| Metric | Rate | Monthly (100 users, 10 chats/day) |
|--------|------|----------------------------------|
| Input tokens | $0.00015/1K | ~$0.50 |
| Output tokens | $0.0006/1K | ~$0.80 |
| **Total** | - | **~$1.30** |

Switching to gpt-3.5-turbo (cheaper):
- Input: $0.0005/1K
- Output: $0.0015/1K
- **Monthly: ~$0.80** (38% savings)

---

## ‚úÖ Quality Checklist

- [ ] AI responses are in Vietnamese
- [ ] Water data is real-time (not cached)
- [ ] Alert thresholds are appropriate
- [ ] Conversation history works (follow-up questions understood)
- [ ] No sensitive data in logs
- [ ] Error messages are user-friendly
- [ ] Response time < 2 seconds
- [ ] API key never exposed in logs
- [ ] Log rotation working (files capped at 5MB)

---

**Last Updated**: November 19, 2024
**Version**: 2.0 - Full AI Enhancement with Conversation History & Logging
